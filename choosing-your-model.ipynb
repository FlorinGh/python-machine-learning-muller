{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to chose your training model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors:\n",
    "\n",
    "**PRO**:\n",
    "* the model is easy to understand\n",
    "* gives reasonable performance without a lot of adjustments\n",
    "* use this algorithm a starting point (a baseline method)  before considering more advanced techniques\n",
    "\n",
    "**AGAINST**:\n",
    "* when the training set is very large, prediction can be slow (this is beacause prediction is running on the actual data)\n",
    "* most of the times the data needs preprocessing\n",
    "* does not perform well on sparse data sets (where features have a lot of zero values)\n",
    "* in practice we usualy have a lot of features so this algorithm is not used that often\n",
    "\n",
    "**CONCLUSION**: Try this algorithm as a first model to have an understanding of the data set; keep the score as a baseline and compare with more complex techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models:\n",
    "\n",
    "**PRO**:\n",
    "* the linear models offer some control over the model: alpha for regression and C for classification (LinearSVC and LogisticRegression); alpha and C control the regularization influence over the model\n",
    "* the regularization can be changed: L1, L2; use L1 to isolate the important features in the model\n",
    "* linear models should be good choice for most of the problems; these models handle well big datasets with sparse data\n",
    "* to get even faster results the solver=\"sag\" could be used with LogisticRegression and Ridge\n",
    "* there is another model that might render good results: SGDClassifier and SGDRegressor\n",
    "\n",
    "**AGAINST**:\n",
    "* linear models do not fit well datasets that have a reduced number of features; it this case more complex models are recommended\n",
    "\n",
    "**CONCLUSION**: Linear models will solve most of your problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees:\n",
    "\n",
    "**PRO**:\n",
    "* the resulting model can be easily visualised and understood by non-experts\n",
    "* the algorithms are completely invariant to scaling of the data: no preprocessing like normalization and standardization is needed; decision trees work well even on features that are on completely different scales, or a mix of binary and continous feartures\n",
    "\n",
    "**AGAINST**:\n",
    "* this model tends to overfit the training data and provide poor generalization performance; left uncontrolled, the model will learn the data by heart meaning will get to pure leafs; to controll overfitting there are two options: pre-pruning means stopping the model early by setting a limit (max_depth, max_leaf_nodes, min_samples_leaf), or post-pruning (removing the overfit data after the model has learn); even with these methods applied the model will still overfit the data\n",
    "\n",
    "**CONCLUSION**: Definitelly should be used, one of the powerful model; use pre-pruining to contain the overfit; if the results are not satisfactory, use ensemble methods instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests:\n",
    "\n",
    "**PRO**:\n",
    "* share the benefits of decision trees with better generalisation\n",
    "* works well without a lot of parameters\n",
    "* doesn't require scaling of data\n",
    "* can easily be parallelized over many CPU cores within a powerful computer\n",
    "\n",
    "**AGAINST**:\n",
    "* this a random model; therefore the random state value will affect the model; the mode trees there are in the forest, the mode robust the model against the choice of the random state\n",
    "* does not perform well on high dimensional, sparse data such as text data (use linear models instead)\n",
    "* require more memory and are slower to train then linear models\n",
    "\n",
    "**PARAMETERS**:\n",
    "* n_jobs: parameter used to adjust the number of cores allocated; n_jobs=-1 will alocate all cores\n",
    "* n_estimators: defines the number of trees; larger is always better, but there are diminishing returns; \n",
    "* max_features: determines how random each tree is, smaller values reduce overfitting; the default values are: sqrt(n_features) for classification and n_features for regression\n",
    "* max_depth / max_leadf_nodes: by default the trees in a random forest are deeper than decision trees; to reduce overfitting this parameter can be used\n",
    "\n",
    "\n",
    "**CONCLUSION**: Random forests for regression and classification are currently among the most widely used machine learning methods. Use them on any data size or type, except text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Decision Trees:\n",
    "\n",
    "**PRO**:\n",
    "* gradient boosted decision trees are among the most powerful and widely used models for supervised learning\n",
    "* similarly to other tree-based models, the algorithm works well without scaling and on a mixture of binary and continuous features\n",
    "\n",
    "**AGAINST**:\n",
    "* the main drawback is that they require careful tuning of the parameters and may take a long time to train\n",
    "* as with other tree-based models, often it doesn't work on high-dimensional sparse data (text)  \n",
    "\n",
    "\n",
    "**PARAMETERS**:\n",
    "* the main parameters are the number of trees, *n_estimators*, and the *learning_rate*, which controls the degree to which each tree is allowed to correct the mistakes of the previous trees\n",
    "* the two parameters are coupled, for a lower learning rate the model needs more trees\n",
    "* the higher the number of trees, the more complex the model and the risk of overfitting\n",
    "* a good practice is to fit the *n_estimators* based on time and budget and then adjust the learning rate to minimize the overfit\n",
    "* another important parameter is *max_depth* to reduce the complexity of each tree; usually this is set very low, often not deeper than 5 splits\n",
    "* if you want to apply gradient boosting to to a large scale problem, it might be worth looking into *xgboost* package and its Python interface\n",
    "\n",
    "\n",
    "**CONCLUSION**: As both gradient boosting and random forests perform well on similar kinds of data, a common approach is to first try random forests, which work quite robustly. If random forests well but prediction time is at a premium, or is important to squeeze out the last percentage of accurachy from the machine learning model, moving to gradient boosting often helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

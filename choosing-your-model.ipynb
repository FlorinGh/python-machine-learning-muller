{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to chose your training model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors:\n",
    "\n",
    "**PRO**:\n",
    "* the model is easy to understand\n",
    "* gives reasonable performance without a lot of adjustments\n",
    "* use this algorithm a starting point (a baseline method)  before considering more advanced techniques\n",
    "\n",
    "**AGAINST**:\n",
    "* when the training set is very large, prediction can be slow (this is beacause prediction is running on the actual data)\n",
    "* most of the times the data needs preprocessing\n",
    "* does not perform well on sparse data sets (where features have a lot of zero values)\n",
    "* in practice we usualy have a lot of features so this algorithm is not used that often\n",
    "\n",
    "**CONCLUSION**: Try this algorithm as a first model to have an understanding of the data set; keep the score as a baseline and compare with more complex techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Models:\n",
    "\n",
    "**PRO**:\n",
    "* the linear models offer some control over the model: alpha for regression and C for classification (LinearSVC and LogisticRegression); alpha and C control the regularization influence over the model\n",
    "* the regularization can be changed: L1, L2; use L1 to isolate the important features in the model\n",
    "* linear models should be good choice for most of the problems; these models handle well big datasets with sparse data\n",
    "* to get even faster results the solver=\"sag\" could be used with LogisticRegression and Ridge\n",
    "* there is another model that might render good results: SGDClassifier and SGDRegressor\n",
    "\n",
    "**AGAINST**:\n",
    "* linear models do not fit well datasets that have a reduced number of features; it this case more complex models are recommended\n",
    "\n",
    "**CONCLUSION**: Linear models will solve most of your problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees:\n",
    "\n",
    "**PRO**:\n",
    "* the resulting model can be easily visualised and understood by non-experts\n",
    "* the algorithms are completely invariant to scaling of the data: no preprocessing like normalization and standardization is needed; decision trees work well even on features that are on completely different scales, or a mix of binary and continous feartures\n",
    "\n",
    "**AGAINST**:\n",
    "* this model tends to overfit the training data and provide poor generalization performance; left uncontrolled, the model will learn the data by heart meaning will get to pure leafs; to controll overfitting there are two options: pre-pruning means stopping the model early by setting a limit (max_depth, max_leaf_nodes, min_samples_leaf), or post-pruning (removing the overfit data after the model has learn); even with these methods applied the model will still overfit the data\n",
    "\n",
    "**CONCLUSION**: Definitelly should be used, one of the powerful model; use pre-pruining to contain the overfit; if the results are not satisfactory, use ensemble methods instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
